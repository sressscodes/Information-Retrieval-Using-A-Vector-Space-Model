{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries and mounting google drive"
      ],
      "metadata": {
        "id": "hORj2J0uPdW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-XAND4BpPdaD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive to access documents\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MFcpYcOLDVq",
        "outputId": "48793326-cbfc-422b-f807-ef6fa0d328b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading texts files from folder"
      ],
      "metadata": {
        "id": "7yYhaEOfP0z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading text files from the specified folder\n",
        "def load_text_files(folder_path):\n",
        "    data = []\n",
        "    doc_id_to_filename = {}\n",
        "    for i, filename in enumerate(os.listdir(folder_path)):\n",
        "        if filename.endswith('.txt'):  # Ensure it's a text file\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data.append(file.read())\n",
        "                doc_id_to_filename[i] = filename\n",
        "    return data, doc_id_to_filename"
      ],
      "metadata": {
        "id": "H8eQo42PQOIo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder path\n",
        "folder_path = '/content/drive/MyDrive/dataset/wk3-docs'"
      ],
      "metadata": {
        "id": "COhTy5CuQfUW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading documents from the folder\n",
        "docs, doc_id_to_filename = load_text_files(folder_path)"
      ],
      "metadata": {
        "id": "MooFinxoQlIx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining sample queries and tokenizing"
      ],
      "metadata": {
        "id": "2l7cL6BNP5f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining sample queries with logical operators\n",
        "queries = [\n",
        "    \"traveling light\", \"plants hate math\", \"thermometer bring a scarf\"\n",
        "]"
      ],
      "metadata": {
        "id": "AUrtFljtQoyi"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the documents and queries by lowercasing and splitting\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "tokenized_docs = [tokenize(doc) for doc in docs]\n",
        "tokenized_queries = [tokenize(query) for query in queries]"
      ],
      "metadata": {
        "id": "OJ0De3nUVtiP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Vocabulary and calculating TF & IDF"
      ],
      "metadata": {
        "id": "zdEvyPJmQDJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building vocabulary (unique words from all documents and queries)\n",
        "vocab = set([word for doc in tokenized_docs for word in doc if word.isalpha()])\n",
        "vocab = sorted(vocab)  # Optional sorting for consistency\n",
        "print(\"Vocabulary:\", vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKJsNVfIeBoi",
        "outputId": "8760e4e7-21bf-434c-a1f3-21ee568da435"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['a', 'accelerating', 'all', 'already', 'always', 'an', 'are', 'astronaut', 'at', 'atoms', 'bacteria', 'bad', 'bags', 'because', 'bed', 'best', 'biologists', 'black', 'break', 'bring', 'charged', 'check', 'chemist', 'chemists', 'clothes', 'coffee', 'did', 'dig', 'dinosaur', 'do', 'down', 'electron', 'electrons', 'elevator', 'fail', 'feeling', 'felt', 'file', 'finding', 'fold', 'gain', 'get', 'go', 'going', 'good', 'goodbye', 'got', 'great', 'had', 'handle', 'hate', 'have', 'he', 'his', 'hole', 'how', 'in', 'it', 'its', 'job', 'jokes', 'keep', 'know', 'ladder', 'leaf', 'little', 'lose', 'lot', 'love', 'make', 'many', 'million', 'mitochondria', 'mix', 'molecules', 'multiply', 'needed', 'negative', 'never', 'new', 'of', 'on', 'paleontologist', 'part', 'particle', 'particles', 'photon', 'physicist', 'physicists', 'plants', 'play', 'police', 'powerhouse', 'problem', 'proton', 'reach', 'react', 'right', 'say', 'scientist', 'scientists', 'so', 'solving', 'some', 'square', 'stairs', 'stars', 'stay', 'take', 'test', 'the', 'their', 'theory', 'thermometer', 'they', 'throw', 'tired', 'to', 'too', 'towards', 'traveling', 'trees', 'trust', 'under', 'unresolved', 'up', 'very', 'viruses', 'wanted', 'was', 'wave', 'were', 'why', 'win', 'with', 'work', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating term frequency (TF)\n",
        "def term_frequency(term, document):\n",
        "  return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "QA88dqSEV5WT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating inverse document frequency (IDF)\n",
        "def inverse_document_frequency(term, all_documents):\n",
        "  num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "  return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "soN2RWi6YRct"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing TF-IDF for a single document\n",
        "def compute_tfidf(document, all_documents, vocab):\n",
        "  tfidf_vector = []\n",
        "  for term in vocab:\n",
        "    tf = term_frequency(term, document)\n",
        "    idf = inverse_document_frequency(term, all_documents)\n",
        "    tfidf_vector.append(tf * idf)\n",
        "  return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "eg3zFSWnV-md"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating cosine similarity and ranking documents"
      ],
      "metadata": {
        "id": "wJAiBEZYQP7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating cosine similarity between two vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  norm_vec1 = np.linalg.norm(vec1)\n",
        "  norm_vec2 = np.linalg.norm(vec2)\n",
        "  return dot_product / (norm_vec1 * norm_vec2)"
      ],
      "metadata": {
        "id": "YR0hi1D0YcVR"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating TF-IDF vectors for both documents and queries\n",
        "doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in tokenized_queries]"
      ],
      "metadata": {
        "id": "VCgwjSnhYuHw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating cosine similarity for each query against the documents\n",
        "cosine_similarities = []\n",
        "for query_vector in query_tfidf_vectors:\n",
        "    similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n",
        "    cosine_similarities.append(similarities)\n",
        "\n",
        "# Displaying cosine similarity results, sorted by highest similarity\n",
        "for i, query in enumerate(queries):\n",
        "    print(f\"\\nTop 3 Cosine similarities for query '{query}':\")\n",
        "\n",
        "    # Pairing document indices with corresponding similarity scores\n",
        "    doc_sim_pairs = list(enumerate(cosine_similarities[i]))\n",
        "\n",
        "    # Sorting the pairs by similarity scores in descending order\n",
        "    doc_sim_pairs_sorted = sorted(doc_sim_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "    # Printing the sorted similarity scores with correct rank\n",
        "    for rank, (doc_idx, similarity) in enumerate(doc_sim_pairs_sorted, start=1):\n",
        "        print(f\"Rank {rank}: Document{doc_idx + 1}: Score: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMpWMmOlZBJd",
        "outputId": "c4897228-339c-4df7-8186-71926c7b2b58"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 3 Cosine similarities for query 'traveling light':\n",
            "Rank 1: Document1: Score: 0.2596\n",
            "Rank 2: Document2: Score: 0.0000\n",
            "Rank 3: Document3: Score: 0.0000\n",
            "\n",
            "Top 3 Cosine similarities for query 'plants hate math':\n",
            "Rank 1: Document9: Score: 0.2849\n",
            "Rank 2: Document10: Score: 0.2213\n",
            "Rank 3: Document1: Score: 0.0000\n",
            "\n",
            "Top 3 Cosine similarities for query 'thermometer bring a scarf':\n",
            "Rank 1: Document7: Score: 0.2916\n",
            "Rank 2: Document5: Score: 0.1293\n",
            "Rank 3: Document8: Score: 0.0092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving result in .txt"
      ],
      "metadata": {
        "id": "8BEfeeEnQmlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the same cosine similarity results to a .txt file\n",
        "with open('cosine_similarities_output.txt', 'w') as file:\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        file.write(f\"\\nTop 3 Cosine similarities for query '{query}':\\n\")\n",
        "\n",
        "        # Pairing document indices with similarity scores\n",
        "        doc_sim_pairs = list(enumerate(cosine_similarities[i]))\n",
        "\n",
        "        # Sorting the pairs by similarity scores in descending order\n",
        "        doc_sim_pairs_sorted = sorted(doc_sim_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "        # Writing the sorted similarities to the file with rank\n",
        "        for rank, (doc_idx, similarity) in enumerate(doc_sim_pairs_sorted, start=1):\n",
        "            file.write(f\"Rank {rank}: Document {doc_idx + 1} Score {similarity:.4f}\\n\")\n",
        "\n",
        "# Output saved in 'cosine_similarities_output.txt'."
      ],
      "metadata": {
        "id": "MakEaFKxMVwI"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}